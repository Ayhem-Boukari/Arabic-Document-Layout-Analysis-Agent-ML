{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311717f0",
   "metadata": {},
   "source": [
    "# 03 - Evaluation\n",
    "Evaluate trained models and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Notebook scaffold - add evaluation code here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3d821",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "This notebook cell embeds the evaluation function used to validate the trained model with Ultralytics from `src/evaluate.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/evaluate.py content\n",
    "from ultralytics import YOLO\n",
    "\n",
    "try:\n",
    "    from src.config import EvalConfig, Paths\n",
    "except Exception:\n",
    "    import sys\n",
    "    sys.path.append('.')\n",
    "    from src.config import EvalConfig, Paths\n",
    "\n",
    "\n",
    "def evaluate(weights: str, data_yaml: str = str(Paths.data_dir / \"data.yaml\"), cfg: EvalConfig = EvalConfig()):\n",
    "    model = YOLO(weights)\n",
    "    metrics = model.val(data=data_yaml, conf=cfg.conf, iou=cfg.iou, project=str(Paths.results_dir), name=\"val\")\n",
    "    return metrics\n",
    "\n",
    "# Example from the notebook:\n",
    "# metrics = evaluate(\"models/best.pt\")\n",
    "# metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
